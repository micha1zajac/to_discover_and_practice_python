{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOZ7MSxVRY/bvKPTEzg8tsa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micha1zajac/to_discover_and_practice_python/blob/main/Python_pandas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6giK3iXY7gt"
      },
      "outputs": [],
      "source": [
        "import pandas\n",
        "import numpy\n",
        "\n",
        "#######################\n",
        "\n",
        "#Plyty = pandas.DataFrame({\"Wykonawca\": ['Queen', 'Eminem', 'Nirvana', 'Green Day', 'Eddie Vedder', 'G-Eazy', 'Bon Jovi', 'Happysad', 'Leonard Cohen', 'Hans Solo', 'Frank Sinatra', \"Rag'N'Bone Man\", \"Eros Ramazzotti\", \"ABBA\", \"Linkin Park\", \"Imagine Dragons\", \"Dżem\", \"Problem\", \"Guns N' Roses\", \"O.S.T.R\", \"VNM\", \"Kortez\", \"Alvaro Soler\", \"Kwiat Jabłoni\", \"Manchester Orchestra\", \"Red Hot Chili Peppers\", \"Pięć Dwa\", \"Małpa\", \"Kings Of Leon\", \"Bastille\", \"Pearl Jam\", \"Hemp Gru\", \"Krzysztof Klenczon\", \"David Gurret\", \"Coldplay\", \"Video\", \"Quebonafide\", \"Zeus\", \"Pezet\", \"Riverside\", \"Miuosh\", \"Gubson\"]}\n",
        " #                )\n",
        "test = pandas.DataFrame()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Basic information\n",
        "\n",
        "df.head() # head print first five rows of df\n",
        "df.info() # info about names of columns, data types\n",
        "df.shape # number of rows and columns (r, c)\n",
        "df.describe() # quick describe of the summary statistic in df\n",
        "df.values # data values in 2d\n",
        "df.columns # list of column names\n",
        "df.index # start, stop and step information\n",
        "\n",
        "## Sorting and subseting\n",
        "\n",
        "df.sort_values(\"column_name\") # arg ascendin,\n",
        "df.sort_values([\"column_name\", \"column_name_2\"], ascending=[False, True]) # sort multiple columns\n",
        "\n",
        "df[\"column_name\"] # slice one column\n",
        "df[['col_name', 'col_name_2']] # slice of multiple column in df\n",
        "\n",
        "df[\"col_name\"] > 50 # subsetting rows, it prints ligical type in every rows in that column\n",
        "df[df[\"col_name\"] > 50] # prints only rows which pass the statement\n",
        "df[(df[\"col_name\"] == \"1\") & (df[\"col_name_2\"] == \"20\")] # sorting for two conditions\n",
        "\n",
        "variable = df[\"col_name\"].isin([\"1\", \"20\"]) # df[].isin() <-- isin funcrion of df object to filter on multiple values of a categorical variable\n",
        "df[variable]\n",
        "\n",
        "## New colums\n",
        "\n",
        "df[\"col_name_new\"] = df[\"col_name\"] / df[\"col_name_2\"] # adding new columns based on calculation on two existing columns\n",
        "\n",
        "## Summary statistics and agregating\n",
        "\n",
        "df[\"col_name\"].mean() # where the center of the data is\n",
        "df[\"col_name\"].median() # the center number of set\n",
        "df[\"col_name\"].mode() # most ofen appear number in set\n",
        "df[\"col_name\"].min()\n",
        "df[\"col_name\"].max()\n",
        "df[\"col_name\"].var() # variance is a squerd deviation from the mean of a random var\n",
        "df[\"col_name\"].std() # standard deviation\n",
        "df[\"col_name\"].sum()\n",
        "df[\"col_name\"].quantile()\n",
        "\n",
        "df[\"col_name\"].agg(\"mean\") # aggregate the data as mean from all rows from column \"col_num\"\n",
        "df[[\"col_name\", \"col_name_2\"]].agg([\"mean\", \"min\"]) # its possible to aggregate also with numpy function\n",
        "\n",
        "df[\"col_name\"].cumsum() # cumulative sum, means cumulative numbers from \"col_name\" - each row increas of another row's value.\n",
        "df[\"col_name\"].cummax()\n",
        "df[\"col_name\"].cummin()\n",
        "df[\"col_name\"].cumprod() # cumulative product, means multiplying values from rows together\n",
        "\n",
        "## Summarize categorical data by counting (avoid double counting)\n",
        "\n",
        "df.drop_duplicates(subset=\"col_name\") # to avoid duplicates in data befour counting use drop_duplicates function\n",
        "\n",
        "df[\"col_name\"].value_counts(sort=True, normalize=True) # count values in \"col_name\" column and sort them or set the narmalization\n",
        "\n",
        "## Grouped summary statistics\n",
        "\n",
        "df.groupby(\"observation_col_name\")[\"col_name\"].mean() # to grupe by some observation, column variable. Select the \"col_name\" and take the mean\n",
        "df.groupby([\"observation_col_name\", \"obserwation_col_name_2\"])[[\"col_name\", \"col_name_2\"]].agg(['min', 'max', 'std']) # combination of this commands are important for ds and looks pretty\n",
        "---------------------------groupby columns---------------------------agregate columns-------------function to agg----\n",
        "\n",
        "# Pivot tables\n",
        "df.pivot_table(values=\"col_name\",                 # pivot table do the same thing lik above command, but is less complicated. Value arg is column which we want to summarize by (above by agregation).\n",
        "               index=\"observation_col_name\",      # index arg is column which we want to group by.\n",
        "               aggfunc=np.median,                 # we can agregate and take summary statistic (median) from index\n",
        "               columns=\"observation_col_name_2\",  # it adds observation col to create pivot table and fill missing values by NaN\n",
        "               fill_value=0,                      # changing NaN values to 0\n",
        "               margins=True)                      # the last column creates mean of rows, except of missing values\n",
        "\n",
        "# Joining data\n",
        "## Inner join = it is returning only the rows of data with matching values in the columns of both tables. Like one-to-many relations.\n",
        "\n",
        "df.merge(df2, on='ward') # it takes first dataframe (df) and marge it with second dataframe (df2) on columnt 'word'\n",
        "df.merge(df2, on+'word', suffixes=('_add_to_repeat_column', '_add_to_2_repeat_column')) # suffixes adds rests to the repeated column names\n",
        "\n",
        "### one-to-one relations = EVERY row in the left table is related to ONLY one row in the right table. Same number of rows after marging.\n",
        "### one-to-many relations = EVERY row in left table is related to one or MORE rows in the right table. After marging this tape of relations number of rows should increased.\n",
        "\n",
        "df.merge(df2, on=('ward')) \\\n",
        "  .merge(df3, on=('ward_2')) \\\n",
        "  .merge(df4, on=('ward_3'))\n",
        "\n",
        "## Left join = it's returning all the rows from left table and matching values from right table for special columns. Like one-to-one relations.\n",
        "df.merge(df2, on=('word'), how='left')\n",
        "\n",
        "## Right join = mirror to the left join\n",
        "df.merge(df2, how='right', left_on='left_word', right_on='right_word' )\n",
        "\n",
        "## Outer join = each of left and right values from paricular columns are merged\n",
        "df.merge(df2, how='outer')\n",
        "\n",
        "## Itself join = its works like df merge with there copy. for example to show the networks between records.\n",
        "df.merge(df, left_on='word', right_on='word_2', suffixes=('_org', '_copy'))\n",
        "\n",
        "# Merging on indexi\n",
        "\n"
      ],
      "metadata": {
        "id": "YzeO5zbkrGLT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}